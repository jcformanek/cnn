{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "present-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-orchestra",
   "metadata": {},
   "source": [
    "## Loading the datasets from image folders & performing transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "located-syndrome",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_path = \"./dataset/train/\"\n",
    "val_data_path = \"./dataset/val/\"\n",
    "test_data_path = \"./dataset/test/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-vector",
   "metadata": {},
   "source": [
    "## Creating the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-shift",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "novel-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 64 * 64, 240)\n",
    "        self.fc2 = nn.Linear(240, 100)\n",
    "        self.fc3 = nn.Linear(100,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3 * 64 * 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-cutting",
   "metadata": {},
   "source": [
    "## Creating the optimizer & loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "institutional-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-walker",
   "metadata": {},
   "source": [
    "## Running the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addressed-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader,\n",
    "    epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, target = batch\n",
    "            inputs = inputs\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item()\n",
    "        training_loss /= len(train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, target = batch\n",
    "            inputs = inputs\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output,target)\n",
    "            valid_loss += loss.data.item()\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1],\n",
    "            target).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader)\n",
    "        \n",
    "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, accuracy = {:.2f}'.format(\n",
    "            epoch, training_loss, valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pediatric-backup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.8991, Validation Loss: 0.9808, accuracy = 0.55\n",
      "Epoch: 1, Training Loss: 0.5792, Validation Loss: 0.8723, accuracy = 0.60\n",
      "Epoch: 2, Training Loss: 0.4506, Validation Loss: 0.7404, accuracy = 0.55\n",
      "Epoch: 3, Training Loss: 0.3040, Validation Loss: 1.0787, accuracy = 0.61\n",
      "Epoch: 4, Training Loss: 0.2439, Validation Loss: 0.9830, accuracy = 0.63\n",
      "Epoch: 5, Training Loss: 0.1703, Validation Loss: 0.9692, accuracy = 0.59\n",
      "Epoch: 6, Training Loss: 0.0860, Validation Loss: 1.1752, accuracy = 0.60\n",
      "Epoch: 7, Training Loss: 0.0566, Validation Loss: 1.0742, accuracy = 0.60\n",
      "Epoch: 8, Training Loss: 0.0360, Validation Loss: 1.3309, accuracy = 0.62\n",
      "Epoch: 9, Training Loss: 0.0272, Validation Loss: 1.2562, accuracy = 0.59\n",
      "Epoch: 10, Training Loss: 0.0239, Validation Loss: 1.3383, accuracy = 0.58\n",
      "Epoch: 11, Training Loss: 0.0280, Validation Loss: 1.2499, accuracy = 0.60\n",
      "Epoch: 12, Training Loss: 0.0136, Validation Loss: 1.5592, accuracy = 0.60\n",
      "Epoch: 13, Training Loss: 0.0109, Validation Loss: 1.5472, accuracy = 0.61\n",
      "Epoch: 14, Training Loss: 0.0061, Validation Loss: 1.6415, accuracy = 0.62\n",
      "Epoch: 15, Training Loss: 0.0040, Validation Loss: 1.5797, accuracy = 0.62\n",
      "Epoch: 16, Training Loss: 0.0029, Validation Loss: 1.6262, accuracy = 0.62\n",
      "Epoch: 17, Training Loss: 0.0023, Validation Loss: 1.7026, accuracy = 0.60\n",
      "Epoch: 18, Training Loss: 0.0020, Validation Loss: 1.7312, accuracy = 0.62\n",
      "Epoch: 19, Training Loss: 0.0017, Validation Loss: 1.7390, accuracy = 0.60\n"
     ]
    }
   ],
   "source": [
    "train(mlp, optimizer, loss_fn, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-bulgaria",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "environmental-resort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "FILENAME = 'dataset/test/cat/cat_6.jpg'\n",
    "labels = ['cat','dog']\n",
    "img = Image.open(FILENAME)\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0) #adds a new dimension at the front of the tensor\n",
    "prediction = mlp(img)\n",
    "prediction = prediction.argmax()\n",
    "print(labels[prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-natural",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gorgeous-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'pretrained_models/mlp.pt'\n",
    "torch.save(mlp.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-friend",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "amended-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mlp_state_dict = torch.load(save_path)\n",
    "mlp.load_state_dict(mlp_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-personal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
